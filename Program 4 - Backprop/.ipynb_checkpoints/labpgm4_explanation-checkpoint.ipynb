{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Program-4 Implementation of ANN\n",
    "\n",
    "* 2 layer fully connected NN\n",
    "* input 2x1, hidden 3x1, output 1x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initilizing input and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2. 9.]\n",
      " [1. 5.]\n",
      " [3. 6.]] (3, 2)\n",
      "[[92.]\n",
      " [86.]\n",
      " [89.]] (3, 1)\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[2,9],[1,5],[3,6]],dtype=float)\n",
    "y = np.array([[92],[86],[89]],dtype=float)\n",
    "print(x,x.shape)\n",
    "print(y,y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize\n",
    "\n",
    "* np.max(x) returns the max value when the parameter x is a array\n",
    "* np.max(x,axis=0) returns the max value of each column, it'll be of size (1 x number_of_columns)\n",
    "* normilization is done so that all the parameters are in the same scale here [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n"
     ]
    }
   ],
   "source": [
    "x = x/np.max(x,axis=0) \n",
    "y = y/100\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid Activation and Sigmoid Gradient\n",
    "\n",
    "* Sigmoid is used as a activation fucntion here to add non-linearity in the network\n",
    "* Gradient of sigmoid is used in Backpropagation step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "def sigmoid_grad(x):\n",
    "    return x*(1-x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper parameters initlization\n",
    "\n",
    "* epoch is the one full forward and backward propagation of the whole input data\n",
    "* eta(or alpha)is the learning rate, it decides how much of a step u take to find the optimal weight values for each epoch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epoch = 1000\n",
    "eta = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Architecture\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_layer_neurons = 2\n",
    "hidden_layer_neurons = 3\n",
    "output_layer_neurons = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Initilization of Weights and Baises\n",
    "\n",
    "* Weigth value should always be random not zero as each neuron should calculate different values when they forward\n",
    "* Bais values can be random or zeros\n",
    "* wh,bh weight and bais from input layer to hidden layer\n",
    "* wout,bout weight and bais from hidden layer to ouput layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wh = np.random.rand(input_layer_neurons,hidden_layer_neurons)\n",
    "bh = np.zeros((1,hidden_layer_neurons))\n",
    "wout = np.random.rand(hidden_layer_neurons,output_layer_neurons)\n",
    "bout = np.zeros((1,output_layer_neurons))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward and Backward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(epoch):\n",
    "    # Forward activations\n",
    "    h_act = sigmoid(np.dot(x,wh)+bh) # hidden_layer_out = input * wt_to_hidden + bias (ip -> hidden)\n",
    "    output = sigmoid(np.dot(h_act,wout)+bout) # output_layer_out = hidden_act * wt_to_output + bias (hidden -> output)\n",
    "    \n",
    "    # Backward calculations of output layer\n",
    "    Error_output = y - output # E_output = y - output_layer_out\n",
    "    output_grad = sigmoid_grad(output) # finding gradient for gradient descent using output_layer_out\n",
    "    delta_output = Error_output * output_grad # delta_output -> derivative to find delta of output\n",
    "    \n",
    "    # Backward calculations of hidden layer\n",
    "    Error_hidden = np.dot(delta_output,wout.T) # E_hidden = delta_output * wt_to_output\n",
    "    hidden_grad = sigmoid_grad(h_act) # same as prev gradient but using hidden_layer_out\n",
    "    delta_hidden = Error_hidden * hidden_grad # delta_hidden -> again derivative to find delta of hidden\n",
    "    \n",
    "    # Updating weights ( wt_to_hidden and wt_to_output ) after error is calculated\n",
    "    wh = wh + np.dot(x.T,delta_hidden)*eta # wt_to_hidden += delta_hidden * X * LR\n",
    "    wout = wout + np.dot(h_act.T,delta_output)*eta # wt_to_output += delta_output * hidden_layer_out * LR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparision of actual and calculated outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Actual Output \n",
      "\n",
      " [[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "\n",
      "\n",
      " Predicted Output \n",
      "\n",
      " [[0.89764584]\n",
      " [0.86919576]\n",
      " [0.90321998]]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n Actual Output \\n\\n\", y)\n",
    "print(\"\\n\\n Predicted Output \\n\\n\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
